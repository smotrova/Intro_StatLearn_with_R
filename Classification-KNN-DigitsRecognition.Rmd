---
title: "K-Nearest Neighbors"
author: "Olena Smotrova"
date: "02/02/2018"
output:
  pdf_document:
    fig_caption: yes
    highlight: tango
    toc: yes
  bookdown::html_document2:
    theme: journal
    highlight: tango
    fig_caption: yes
    toc: yes
    number_sections: no
bibliography: library.bib
link-citations: yes
csl: advanced-optical-materials.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Classification. K-Nearest neighbors.

Predicting a qualitative responses for an observation can be referred to as _classifying_ that observation, since it involves assigning the observation to a category.

Many approaches attempt to estimate the conditional distribution of $Y$ given $X$, and then classify a given observation to the class with highest estimated probability. One such method is the _K-nearest neighbors_ (KNN) classifier. Given a positive integer $K$ and a test observation $x_{0}$, represented by $N_{0}$. It then estimates the conditional probability for class $j$ as the fraction of points $N_{0}$ whose response values equal $j$:

$$Pr(Y=j|X=x_{0})=\frac{1}{K}\displaystyle\sum_{i \in N_{0}} I(y_{i} = j)$$

Finally, KNN applies Bayes rule and classifies the test observation $x_{0}$ to the class with the largest probability. There is not a strong relationship between the _training error rate_ and _test error rate_. With $K=1$, the KNN training error rate equals $0$, but the test error rate may be quite high. In general, as we use more flexible classification methods, the training error will decline but the test error rate may not. As $K$ decreases, the method becomes more flexible. 

# Improving Performance.
## Cross-Validation.

To estimate the error of the KNN for a particular $K$ we could use _n-fold cross-validation_ (n-fold CV)

* Divide the training set into $n$ equal pieces: $S_{1}, S_{2}, ... S_{n}$
* Classify each point in $S_{i}$ using KNN with training set $S- S_{i}$,  $i = 1,...,n$ 
* Define $\epsilon_{i}$ as a fraction of $S_{i}$ that is incorrectly classified
* Take the average estimated error KNN = $\frac{1}{n}\displaystyle\sum_{i=1}^{n} \epsilon_{i}$

An extreme type of cross-validation is n-fold CV on a training set of size _n_. If we want to estimate the error of KNN, this amounts to classifying each training point by running KNN on the remaining ( _n-1_) points, and then looking at the fraction of mistakes made. It is commonly called _leave-one-out-cross-validation_ (LOOCV).

## Choose a metric
Let $X$ be the space in winch data lie. A distance function $d: X\times X \to \mathbb{R}$ is a _metric_ if it is satisfies following properties:

* $d(x,y)\geq 0$ (nonnegativity)
* $d(x,y)=0 \Leftrightarrow x=y$
* $d(x,y)= d(y,x)$ (symmetry)
* $d(x,z) \leq d(x,y)+d(y,z)$ (triangle inequality)

Measuring distance in $\mathbb{R}^{m}$ is $l_{p}$ distance, $p\geq1$:
$$\parallel x-y \parallel_{p}= \sqrt[p]{\sum_{i=1}^{m}|x_{i}-y_{i}|^p }$$

* $l_{2}$ distance is Euclidean distance (usual choice)
* $l_{\infty}$ distance: $\parallel x-y \parallel_{\infty}= max_{i}|x_{i}-y_{i}|$

# Data
## Description

MNIST ("Modified National Institute of Standards and Technology") is the de facto ?hello world? data set of computer vision. Since its release in 1999, this classic data set of handwritten images has served as the basis for bench marking classification algorithms.

The data file contains gray-scale images of hand-drawn digits, from zero through nine.

Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive. Each row in data set represents an image stretched into a vector with 784 coordinates.

The data set, ([train.csv](https://www.kaggle.com/c/digit-recognizer/data)), has 785 columns. The first column, called _label_, is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.

Each pixel column in the data set has a name like $pixelx$, where $x$ is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed $x$ as $x = i * 28 + j$, where $i$ and $j$ are integers between 0 and 27, inclusive. Then $pixelx$ is located on row $i$ and column $j$ of a 28 x 28 matrix, (indexing by zero). For example, $pixel31$ indicates the pixel that is in the fourth column from the left, and the second row from the top. [Data source](https://www.kaggle.com/c/digit-recognizer) (_Kaggle, 2012_)

```{r, echo=FALSE, message=FALSE, warning=FALSE}
setwd("D:/Work/LENA/edX/R-practice/UCSanDiegoX-MachineLearningFundamentals")
digits <- read.csv("./Data/DigitsRecognition.csv")
str(digits, list.len=10)
```

## Visualization

There are some randomly selected examples of handwritten digits and their labels from the data set.


```{r, echo=FALSE, fig.height=2, fig.width=10, message=FALSE, warning=FALSE}
library(ggplot2)
library(ggthemes)
library(gridExtra)

DigitToPlot <- function(n) {
  data_to_plot = as.numeric(digits[n, 2:ncol(digits)])
  x = c()
  y = c()
  z = c()
  
  for (i in seq(1, length(data_to_plot)) ) {
    x = c(x, (i-1)%%28)
    y = c(y, 28-(i-1)%/%28) 
    z = c(z, data_to_plot[i]/255)
  }
  
  d = cbind.data.frame(x,y,z)
  return(d)
}

DigitPlot <- function(n) {
    p = ggplot(DigitToPlot(n), aes(x, y)) + geom_raster(aes(fill = z) ) +
      coord_equal() + theme_void() +
        ggtitle(label = as.character(digits[n, 1])) +
          scale_fill_gradient(low = "black", high = "white") +
            theme(legend.position="none", plot.title = element_text(hjust = 0.5, vjust = 0.2))
  
  return(p)
}

n = sample(1:nrow(digits), 5)

grid.arrange(DigitPlot(n[1]),
             DigitPlot(n[2]),
             DigitPlot(n[3]),
             DigitPlot(n[4]),
             DigitPlot(n[5]),nrow=1)

```

# KNN in R

We  will perform KNN using the $knn()$ function, winch is part of _class_ library. The function requires

* A matrix or a data frame of training set cases
* A matrix or a data frame of test set cases
* A vector containing the class labels for the training observations
* A value for K, the number of nearest neighbors to be used by classifier


```{r, message=FALSE, warning=FALSE}
library(caTools)
library(class)
```

We use $sample.split()$ function from _caTools_ to split data set randomly into training and test sets.

```{r}
set.seed(1000)
spl = sample.split(digits$label, 0.75)

digitsTrain = subset(digits, spl == TRUE)
digitsTest = subset(digits, spl == FALSE)

labelTrain = digitsTrain$label
```

_Note_. A seed must be set in order to ensure reproducibility of results. In $knn()$ if several observations are tied as nearest neighbors, R randomly break the tie.

```{r}
knn.pred = knn(digitsTrain[ ,2:785], digitsTest[ , 2:785], labelTrain, k = 3 )
```

Error test rate defines as a fraction of incorrectly classified observations.
```{r}
table(knn.pred, digitsTest$label)
mean(knn.pred == digitsTest$label)
```

The results of KNN with K=3 are very good for these data. Visualizations below show five randomly selected examples of digits with labels defined by KNN classifier. 

```{r, echo=FALSE, fig.height=2, fig.width=10, message=FALSE, warning=FALSE}

DigitToPlot <- function(n) {
  data_to_plot = as.numeric(digitsTest[n, 2:ncol(digitsTest)])
  x = c()
  y = c()
  z = c()
  
  for (i in seq(1, length(data_to_plot)) ) {
    x = c(x, (i-1)%%28)
    y = c(y, 28-(i-1)%/%28) 
    z = c(z, data_to_plot[i]/255)
  }
  
  d = cbind.data.frame(x,y,z)
  return(d)
}


DigitPlot <- function(n) {
  p = ggplot(DigitToPlot(n), aes(x, y)) + geom_raster(aes(fill = z) ) +
    coord_equal() + theme_void() +
    ggtitle(label = as.character(knn.pred[n])) +
    scale_fill_gradient(low = "black", high = "white") +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5, vjust = 0.2))
  
  return(p)
}

n = sample(1:nrow(digitsTest), 5)
n1 = sample(which(knn.pred != digitsTest$label), 1)
n[3] = n1
grid.arrange(DigitPlot(n[1]),
             DigitPlot(n[2]),
             DigitPlot(n[3]),
             DigitPlot(n[4]),
             DigitPlot(n[5]),nrow=1)
```


